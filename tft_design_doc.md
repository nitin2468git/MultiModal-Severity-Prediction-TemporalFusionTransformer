# Multi-Modal COVID-19 Severity Prediction using Temporal Fusion Transformer
## Design Documentation

### 1. Project Structure and Development Workflow

#### 1.1 Repository Structure
```
covid19-tft-severity-prediction/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ .gitignore
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ model_config.yaml
â”‚   â”œâ”€â”€ data_config.yaml
â”‚   â””â”€â”€ training_config.yaml
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # Original Synthea CSV files
â”‚   â”œâ”€â”€ processed/              # Cleaned and preprocessed data
â”‚   â”œâ”€â”€ features/               # Engineered features
â”‚   â””â”€â”€ splits/                 # Train/val/test splits
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ synthea_loader.py   # Load and parse CSV files
â”‚   â”‚   â”œâ”€â”€ timeline_builder.py # Create patient timelines
â”‚   â”‚   â”œâ”€â”€ feature_engineer.py # Feature engineering
â”‚   â”‚   â””â”€â”€ data_validator.py   # Data quality checks
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ tft_model.py        # TFT implementation
â”‚   â”‚   â”œâ”€â”€ baseline_models.py  # LSTM, GRU baselines
â”‚   â”‚   â””â”€â”€ loss_functions.py   # Custom loss functions
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ trainer.py          # Training orchestration
â”‚   â”‚   â”œâ”€â”€ callbacks.py        # Custom callbacks
â”‚   â”‚   â””â”€â”€ metrics.py          # Evaluation metrics
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ evaluator.py        # Model evaluation
â”‚   â”‚   â”œâ”€â”€ attention_analysis.py # Attention visualization
â”‚   â”‚   â””â”€â”€ clinical_validation.py # Clinical metrics
â”‚   â”œâ”€â”€ visualization/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ plots.py            # Plotting utilities
â”‚   â”‚   â””â”€â”€ dashboard.py        # Interactive dashboard
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ config.py           # Configuration management
â”‚       â”œâ”€â”€ logging.py          # Logging setup
â”‚       â””â”€â”€ helpers.py          # Utility functions
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_feature_engineering.ipynb
â”‚   â”œâ”€â”€ 03_model_development.ipynb
â”‚   â”œâ”€â”€ 04_evaluation_analysis.ipynb
â”‚   â””â”€â”€ 05_results_visualization.ipynb
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ baseline_experiments/
â”‚   â”œâ”€â”€ tft_experiments/
â”‚   â””â”€â”€ results/
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_data_processing.py
â”‚   â”œâ”€â”€ test_models.py
â”‚   â””â”€â”€ test_evaluation.py
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture.md
â”‚   â”œâ”€â”€ design.md
â”‚   â”œâ”€â”€ api_documentation.md
â”‚   â””â”€â”€ deployment_guide.md
â””â”€â”€ scripts/
    â”œâ”€â”€ run_preprocessing.py
    â”œâ”€â”€ run_training.py
    â”œâ”€â”€ run_evaluation.py
    â””â”€â”€ generate_results.py
```

#### 1.2 Development Phases (4-Day Sprint)

```
Day 1: Data Pipeline Development
â”œâ”€â”€ Morning: Environment setup + data exploration
â”œâ”€â”€ Afternoon: Implement data loading and preprocessing
â””â”€â”€ Evening: Feature engineering and validation

Day 2: Model Implementation
â”œâ”€â”€ Morning: TFT model implementation
â”œâ”€â”€ Afternoon: Training pipeline and loss functions
â””â”€â”€ Evening: Baseline models for comparison

Day 3: Training and Evaluation
â”œâ”€â”€ Morning: Model training and hyperparameter tuning
â”œâ”€â”€ Afternoon: Evaluation pipeline and metrics
â””â”€â”€ Evening: Attention analysis and interpretability

Day 4: Results and Documentation
â”œâ”€â”€ Morning: Generate final results and visualizations
â”œâ”€â”€ Afternoon: Write paper and create presentation
â””â”€â”€ Evening: Code cleanup and documentation
```

### 2. Core Component Design

#### 2.1 Data Processing Pipeline Design

```python
# src/data/synthea_loader.py
class SyntheaLoader:
    """Load and parse Synthea COVID-19 CSV files"""
    
    def __init__(self, data_path: str):
        self.data_path = data_path
        self.tables = {}
        
    def load_all_tables(self) -> Dict[str, pd.DataFrame]:
        """Load all CSV files into memory"""
        csv_files = [
            'patients.csv', 'observations.csv', 'medications.csv',
            'procedures.csv', 'devices.csv', 'encounters.csv',
            'conditions.csv', 'careplans.csv', 'immunizations.csv'
        ]
        
        for file in csv_files:
            table_name = file.replace('.csv', '')
            self.tables[table_name] = pd.read_csv(f"{self.data_path}/{file}")
            
        return self.tables
    
    def get_covid_patients(self) -> List[str]:
        """Filter patients with COVID-19 diagnosis"""
        covid_conditions = self.tables['conditions']
        covid_patients = covid_conditions[
            covid_conditions['DESCRIPTION'].str.contains('COVID', case=False, na=False)
        ]['PATIENT'].unique()
        
        return covid_patients.tolist()

# src/data/timeline_builder.py
class TimelineBuilder:
    """Build temporal sequences for each patient"""
    
    def __init__(self, tables: Dict[str, pd.DataFrame]):
        self.tables = tables
        
    def build_patient_timeline(self, patient_id: str, 
                             start_date: str = None,
                             end_date: str = None) -> pd.DataFrame:
        """Create unified timeline for a patient"""
        
        timeline_events = []
        
        # Add vital signs and observations
        obs = self.tables['observations'][
            self.tables['observations']['PATIENT'] == patient_id
        ]
        for _, row in obs.iterrows():
            timeline_events.append({
                'patient_id': patient_id,
                'timestamp': pd.to_datetime(row['DATE']),
                'event_type': 'observation',
                'code': row['CODE'],
                'description': row['DESCRIPTION'],
                'value': row['VALUE'],
                'units': row['UNITS']
            })
            
        # Add medications
        meds = self.tables['medications'][
            self.tables['medications']['PATIENT'] == patient_id
        ]
        for _, row in meds.iterrows():
            timeline_events.append({
                'patient_id': patient_id,
                'timestamp': pd.to_datetime(row['START']),
                'event_type': 'medication_start',
                'code': row['CODE'],
                'description': row['DESCRIPTION'],
                'value': 1,  # Binary: medication started
                'units': None
            })
            
        # Similar for procedures, devices, etc.
        
        # Convert to DataFrame and sort by timestamp
        timeline_df = pd.DataFrame(timeline_events)
        timeline_df = timeline_df.sort_values('timestamp')
        
        return timeline_df
    
    def create_time_series_features(self, timeline_df: pd.DataFrame,
                                  time_interval: str = '1H') -> pd.DataFrame:
        """Convert events to regular time series with specified interval"""
        
        # Create time grid
        start_time = timeline_df['timestamp'].min()
        end_time = timeline_df['timestamp'].max()
        time_grid = pd.date_range(start=start_time, end=end_time, freq=time_interval)
        
        # Initialize feature matrix
        features = pd.DataFrame(index=time_grid)
        features['patient_id'] = timeline_df['patient_id'].iloc[0]
        
        # Pivot observations to columns
        obs_pivot = timeline_df[timeline_df['event_type'] == 'observation'].pivot_table(
            index='timestamp', columns='description', values='value', aggfunc='mean'
        )
        
        # Resample to regular intervals and forward fill
        obs_resampled = obs_pivot.resample(time_interval).mean().fillna(method='ffill')
        
        # Merge with features
        features = features.join(obs_resampled, how='left')
        
        return features

# src/data/feature_engineer.py
class FeatureEngineer:
    """Engineer domain-specific features"""
    
    def __init__(self):
        self.feature_configs = self._load_feature_configs()
        
    def engineer_static_features(self, patient_data: pd.DataFrame) -> pd.DataFrame:
        """Create static patient features"""
        
        features = pd.DataFrame()
        
        # Demographic features
        features['age'] = patient_data['age']
        features['gender'] = pd.get_dummies(patient_data['gender'])['M']
        features['race_white'] = (patient_data['race'] == 'white').astype(int)
        
        # Comorbidity features (from conditions table)
        comorbidities = ['diabetes', 'hypertension', 'obesity', 'copd', 'heart_disease']
        for condition in comorbidities:
            features[f'has_{condition}'] = patient_data[condition].fillna(0)
            
        # Socioeconomic features
        features['healthcare_coverage'] = patient_data['healthcare_coverage']
        features['income_level'] = patient_data['income'] / 1000  # Scale income
        
        return features
    
    def engineer_temporal_features(self, timeline_df: pd.DataFrame) -> pd.DataFrame:
        """Create time-varying features"""
        
        features = timeline_df.copy()
        
        # Vital sign features
        if 'heart_rate' in features.columns:
            features['hr_rolling_mean_6h'] = features['heart_rate'].rolling('6H').mean()
            features['hr_rolling_std_6h'] = features['heart_rate'].rolling('6H').std()
            features['hr_trend'] = features['heart_rate'].diff()
            
        if 'oxygen_saturation' in features.columns:
            features['o2_sat_below_90'] = (features['oxygen_saturation'] < 90).astype(int)
            features['o2_sat_trend'] = features['oxygen_saturation'].diff()
            
        # Medication features
        covid_medications = ['remdesivir', 'dexamethasone', 'tocilizumab']
        for med in covid_medications:
            if f'{med}_active' in features.columns:
                features[f'{med}_duration'] = features[f'{med}_active'].cumsum()
                
        # Severity indicators
        features['ventilator_support'] = features.get('mechanical_ventilator', 0)
        features['icu_stay'] = features.get('icu_admission', 0)
        
        # Time-based features
        features['hour_of_day'] = features.index.hour
        features['day_of_week'] = features.index.dayofweek
        features['days_since_admission'] = (features.index - features.index[0]).days
        
        return features
```

#### 2.2 Model Implementation Design

```python
# src/models/tft_model.py
import torch
import torch.nn as nn
from pytorch_forecasting import TemporalFusionTransformer
from pytorch_forecasting.data import TimeSeriesDataSet

class COVID19TFT(nn.Module):
    """Custom TFT for COVID-19 severity prediction"""
    
    def __init__(self, config: dict):
        super().__init__()
        self.config = config
        
        # Initialize TFT backbone
        self.tft = TemporalFusionTransformer.from_dataset(
            dataset=None,  # Will be set during training
            learning_rate=config['learning_rate'],
            hidden_size=config['hidden_size'],
            attention_head_size=config['attention_head_size'],
            dropout=config['dropout'],
            hidden_continuous_size=config['hidden_continuous_size'],
            output_size=1,  # Will modify for multi-task
            reduce_on_plateau_patience=config['patience']
        )
        
        # Multi-task prediction heads
        self.mortality_head = nn.Sequential(
            nn.Linear(config['hidden_size'], 64),
            nn.ReLU(),
            nn.Dropout(config['dropout']),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )
        
        self.icu_head = nn.Sequential(
            nn.Linear(config['hidden_size'], 64),
            nn.ReLU(),
            nn.Dropout(config['dropout']),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )
        
        self.ventilator_head = nn.Sequential(
            nn.Linear(config['hidden_size'], 64),
            nn.ReLU(),
            nn.Dropout(config['dropout']),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )
        
        self.los_head = nn.Sequential(
            nn.Linear(config['hidden_size'], 64),
            nn.ReLU(),
            nn.Dropout(config['dropout']),
            nn.Linear(64, 1),
            nn.ReLU()  # Positive values only
        )
        
    def forward(self, x):
        # Get TFT embeddings
        tft_output = self.tft(x)
        embeddings = tft_output['prediction']  # [batch, hidden_size]
        
        # Multi-task predictions
        mortality_pred = self.mortality_head(embeddings)
        icu_pred = self.icu_head(embeddings)
        ventilator_pred = self.ventilator_head(embeddings)
        los_pred = self.los_head(embeddings)
        
        return {
            'mortality': mortality_pred,
            'icu_admission': icu_pred,
            'ventilator_need': ventilator_pred,
            'length_of_stay': los_pred,
            'attention_weights': tft_output.get('attention', None)
        }

# src/models/loss_functions.py
class MultiTaskLoss(nn.Module):
    """Custom multi-task loss function"""
    
    def __init__(self, task_weights: dict = None):
        super().__init__()
        self.task_weights = task_weights or {
            'mortality': 2.0,      # High weight for mortality
            'icu_admission': 1.5,
            'ventilator_need': 1.5,
            'length_of_stay': 1.0
        }
        
        # Loss functions for each task
        self.bce_loss = nn.BCELoss()
        self.mse_loss = nn.MSELoss()
        self.huber_loss = nn.HuberLoss()
        
    def forward(self, predictions: dict, targets: dict):
        losses = {}
        
        # Mortality loss (binary classification)
        if 'mortality' in targets:
            losses['mortality'] = self.bce_loss(
                predictions['mortality'].squeeze(),
                targets['mortality'].float()
            )
            
        # ICU admission loss (binary classification)
        if 'icu_admission' in targets:
            losses['icu_admission'] = self.bce_loss(
                predictions['icu_admission'].squeeze(),
                targets['icu_admission'].float()
            )
            
        # Ventilator need loss (binary classification)
        if 'ventilator_need' in targets:
            losses['ventilator_need'] = self.bce_loss(
                predictions['ventilator_need'].squeeze(),
                targets['ventilator_need'].float()
            )
            
        # Length of stay loss (regression)
        if 'length_of_stay' in targets:
            losses['length_of_stay'] = self.huber_loss(
                predictions['length_of_stay'].squeeze(),
                targets['length_of_stay'].float()
            )
            
        # Compute weighted total loss
        total_loss = sum(
            self.task_weights[task] * loss 
            for task, loss in losses.items()
        )
        
        losses['total'] = total_loss
        return losses
```

#### 2.3 Training Pipeline Design

```python
# src/training/trainer.py
import pytorch_lightning as pl
from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint
from pytorch_lightning.loggers import TensorBoardLogger

class COVID19TFTTrainer(pl.LightningModule):
    """PyTorch Lightning trainer for TFT model"""
    
    def __init__(self, model: COVID19TFT, config: dict):
        super().__init__()
        self.model = model
        self.config = config
        self.loss_fn = MultiTaskLoss(config.get('task_weights'))
        
        # Metrics tracking
        self.train_metrics = {}
        self.val_metrics = {}
        
    def training_step(self, batch, batch_idx):
        predictions = self.model(batch['features'])
        losses = self.loss_fn(predictions, batch['targets'])
        
        # Log individual task losses
        for task, loss in losses.items():
            self.log(f'train_{task}_loss', loss, prog_bar=True)
            
        return losses['total']
    
    def validation_step(self, batch, batch_idx):
        predictions = self.model(batch['features'])
        losses = self.loss_fn(predictions, batch['targets'])
        
        # Log validation losses
        for task, loss in losses.items():
            self.log(f'val_{task}_loss', loss)
            
        # Compute metrics (AUROC, AUPRC, etc.)
        metrics = self.compute_metrics(predictions, batch['targets'])
        for metric_name, value in metrics.items():
            self.log(f'val_{metric_name}', value)
            
        return losses['total']
    
    def configure_optimizers(self):
        optimizer = torch.optim.AdamW(
            self.parameters(),
            lr=self.config['learning_rate'],
            weight_decay=self.config.get('weight_decay', 0.01)
        )
        
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer,
            mode='min',
            patience=self.config.get('lr_patience', 5),
            factor=0.5
        )
        
        return {
            'optimizer': optimizer,
            'lr_scheduler': scheduler,
            'monitor': 'val_total_loss'
        }
    
    def compute_metrics(self, predictions, targets):
        """Compute evaluation metrics"""
        from sklearn.metrics import roc_auc_score, average_precision_score
        
        metrics = {}
        
        # Binary classification metrics
        for task in ['mortality', 'icu_admission', 'ventilator_need']:
            if task in predictions and task in targets:
                y_true = targets[task].cpu().numpy()
                y_pred = predictions[task].detach().cpu().numpy()
                
                metrics[f'{task}_auroc'] = roc_auc_score(y_true, y_pred)
                metrics[f'{task}_auprc'] = average_precision_score(y_true, y_pred)
                
        # Regression metrics
        if 'length_of_stay' in predictions and 'length_of_stay' in targets:
            y_true = targets['length_of_stay'].cpu().numpy()
            y_pred = predictions['length_of_stay'].detach().cpu().numpy()
            
            metrics['los_mae'] = np.mean(np.abs(y_true - y_pred))
            metrics['los_rmse'] = np.sqrt(np.mean((y_true - y_pred) ** 2))
            
        return metrics

# Main training function
def train_model(config: dict):
    """Main training orchestration"""
    
    # Setup data
    data_module = COVID19DataModule(config)
    
    # Initialize model
    model = COVID19TFT(config)
    trainer_module = COVID19TFTTrainer(model, config)
    
    # Setup callbacks
    early_stopping = EarlyStopping(
        monitor='val_total_loss',
        patience=config.get('patience', 10),
        mode='min'
    )
    
    checkpoint_callback = ModelCheckpoint(
        monitor='val_total_loss',
        save_top_k=3,
        mode='min',
        filename='tft-{epoch:02d}-{val_total_loss:.3f}'
    )
    
    # Setup logger
    logger = TensorBoardLogger('logs', name='covid19_tft')
    
    # Initialize trainer
    trainer = pl.Trainer(
        max_epochs=config['max_epochs'],
        callbacks=[early_stopping, checkpoint_callback],
        logger=logger,
        accelerator='gpu' if torch.cuda.is_available() else 'cpu',
        devices=1,
        precision=16 if config.get('use_mixed_precision', True) else 32
    )
    
    # Train model
    trainer.fit(trainer_module, data_module)
    
    return trainer_module, trainer
```

### 3. Configuration Management

```python
# configs/model_config.yaml
model:
  name: "COVID19TFT"
  hidden_size: 128
  attention_head_size: 4
  num_encoder_layers: 3
  num_decoder_layers: 3
  dropout: 0.1
  learning_rate: 0.001
  weight_decay: 0.01
  
multi_task:
  task_weights:
    mortality: 2.0
    icu_admission: 1.5
    ventilator_need: 1.5
    length_of_stay: 1.0
    
temporal:
  lookback_window: 168  # 7 days in hours
  forecast_horizon: 72  # 3 days in hours
  time_interval: "1H"
  max_sequence_length: 720  # 30 days

# configs/training_config.yaml
training:
  batch_size: 32
  max_epochs: 100
  patience: 15
  lr_patience: 5
  use_mixed_precision: true
  gradient_clip_val: 1.0
  
validation:
  val_split: 0.2
  test_split: 0.1
  stratify_by: "mortality"
  
data_augmentation:
  temporal_jitter: 0.1
  feature_noise: 0.05
  enabled: true
```

### 4. Evaluation and Analysis Framework

```python
# src/evaluation/evaluator.py
class ModelEvaluator:
    """Comprehensive model evaluation"""
    
    def __init__(self, model, data_loader, config):
        self.model = model
        self.data_loader = data_loader
        self.config = config
        
    def evaluate_model(self):
        """Run complete evaluation pipeline"""
        
        results = {
            'performance_metrics': self.compute_performance_metrics(),
            'calibration_analysis': self.analyze_calibration(),
            'attention_analysis': self.analyze_attention_patterns(),
            'clinical_validation': self.clinical_validation(),
            'failure_analysis': self.analyze_failures()
        }
        
        return results
    
    def compute_performance_metrics(self):
        """Compute standard ML metrics"""
        
        all_predictions = []
        all_targets = []
        
        self.model.eval()
        with torch.no_grad():
            for batch in self.data_loader:
                predictions = self.model(batch['features'])
                all_predictions.append(predictions)
                all_targets.append(batch['targets'])
                
        # Aggregate results
        metrics = {}
        
        # Binary classification metrics
        for task in ['mortality', 'icu_admission', 'ventilator_need']:
            y_true = torch.cat([t[task] for t in all_targets]).cpu().numpy()
            y_pred = torch.cat([p[task] for p in all_predictions]).cpu().numpy()
            
            metrics[task] = {
                'auroc': roc_auc_score(y_true, y_pred),
                'auprc': average_precision_score(y_true, y_pred),
                'sensitivity': recall_score(y_true, y_pred > 0.5),
                'specificity': recall_score(1 - y_true, y_pred <= 0.5),
                'ppv': precision_score(y_true, y_pred > 0.5),
                'npv': precision_score(1 - y_true, y_pred <= 0.5)
            }
            
        return metrics
    
    def analyze_attention_patterns(self):
        """Analyze temporal attention patterns"""
        
        attention_weights = []
        patient_characteristics = []
        
        self.model.eval()
        with torch.no_grad():
            for batch in self.data_loader:
                predictions = self.model(batch['features'])
                if 'attention_weights' in predictions:
                    attention_weights.append(predictions['attention_weights'])
                    patient_characteristics.append(batch['static_features'])
                    
        # Analyze patterns
        analysis = {
            'temporal_focus': self.analyze_temporal_focus(attention_weights),
            'feature_importance': self.analyze_feature_importance(attention_weights),
            'patient_specific_patterns': self.analyze_patient_patterns(
                attention_weights, patient_characteristics
            )
        }
        
        return analysis

# src/visualization/plots.py
class ResultsPlotter:
    """Generate publication-quality plots"""
    
    def __init__(self, results):
        self.results = results
        
    def plot_roc_curves(self, save_path=None):
        """Plot ROC curves for all tasks"""
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        axes = axes.ravel()
        
        tasks = ['mortality', 'icu_admission', 'ventilator_need']
        for i, task in enumerate(tasks):
            if task in self.results['roc_data']:
                fpr = self.results['roc_data'][task]['fpr']
                tpr = self.results['roc_data'][task]['tpr']
                auc = self.results['performance_metrics'][task]['auroc']
                
                axes[i].plot(fpr, tpr, label=f'AUC = {auc:.3f}')
                axes[i].plot([0, 1], [0, 1], 'k--', alpha=0.5)
                axes[i].set_xlabel('False Positive Rate')
                axes[i].set_ylabel('True Positive Rate')
                axes[i].set_title(f'{task.replace("_", " ").title()} ROC Curve')
                axes[i].legend()
                axes[i].grid(True, alpha=0.3)
                
        plt.tight_layout()
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
    
    def plot_attention_heatmap(self, patient_idx=0, save_path=None):
        """Plot attention weights over time for a specific patient"""
        
        attention_data = self.results['attention_analysis']['patient_specific_patterns'][patient_idx]
        
        plt.figure(figsize=(15, 8))
        sns.heatmap(
            attention_data['attention_matrix'],
            xticklabels=attention_data['time_labels'],
            yticklabels=attention_data['feature_labels'],
            cmap='Blues',
            cbar_kws={'label': 'Attention Weight'}
        )
        plt.title(f'Temporal Attention Pattern - Patient {patient_idx}')
        plt.xlabel('Time Steps')
        plt.ylabel('Features')
        plt.xticks(rotation=45)
        plt.yticks(rotation=0)
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
```

### 5. Rapid Development Workflow

#### 5.1 Quick Start Scripts

```python
# scripts/quick_start.py
"""Quick start script for 4-day development"""

def day1_data_pipeline():
    """Day 1: Set up data pipeline"""
    print("ðŸš€ Day 1: Data Pipeline Development")
    
    # Load and explore data
    loader = SyntheaLoader('data/raw/')
    tables = loader.load_all_tables()
    covid_patients = loader.get_covid_patients()
    
    print(f"Found {len(covid_patients)} COVID patients")
    
    # Build timelines for first 100 patients (for speed)
    timeline_builder = TimelineBuilder(tables)
    sample_patients = covid_patients[:100]
    
    processed_data = []
    for patient_id in tqdm(sample_patients):
        timeline = timeline_builder.build_patient_timeline(patient_id)
        features = timeline_builder.create_time_series_features(timeline)
        processed_data.append(features)
    
    # Save processed data
    with open('data/processed/day1_timelines.pkl', 'wb') as f:
        pickle.dump(processed_data, f)
    
    print("âœ… Day 1 Complete: Data pipeline ready")

def day2_model_implementation():
    """Day 2: Implement TFT model"""
    print("ðŸš€ Day 2: Model Implementation")
    
    # Load processed data
    with open('data/processed/day1_timelines.pkl', 'rb') as f:
        processed_data = pickle.load(f)
    
    # Create dataset
    dataset = COVID19Dataset(processed_data)
    
    # Initialize model
    config = load_config('configs/model_config.yaml')
    model = COVID19TFT(config)
    
    # Quick training test (1 epoch)
    trainer = COVID19TFTTrainer(model, config)
    quick_trainer = pl.Trainer(max_epochs=1, fast_dev_run=True)
    quick_trainer.fit(trainer, dataset)
    
    print("âœ… Day 2 Complete: Model implementation ready")

# Similar for days 3 and 4...
```

#### 5.2 Testing Strategy

```python
# tests/test_data_processing.py
def test_synthea_loader():
    """Test data loading functionality"""
    loader = SyntheaLoader('data/raw/')
    tables = loader.load_all_tables()
    
    assert 'patients' in tables
    assert 'observations' in tables
    assert len(tables['patients']) > 0
    
def test_timeline_builder():
    """Test timeline construction"""
    # Mock data for testing
    mock_tables = create_mock_synthea_data()
    builder = TimelineBuilder(mock_tables)
    
    timeline = builder.build_patient_timeline('test_patient_001')
    assert len(timeline) > 0
    assert 'timestamp' in timeline.columns
    
def test_feature_engineering():
    """Test feature engineering pipeline"""
    mock_timeline = create_mock_timeline()
    engineer = FeatureEngineer()
    
    features = engineer.engineer_temporal_features(mock_timeline)
    assert 'hr_rolling_mean_6h' in features.columns
    assert 'days_since_admission' in features.columns
```

### 6. Implementation Checklist and Milestones

#### 6.1 Day-by-Day Implementation Checklist

**Day 1: Data Foundation (8 hours)**
```
Hour 1-2: Environment Setup
â”œâ”€â”€ âœ… Clone repository structure
â”œâ”€â”€ âœ… Install dependencies (requirements.txt)
â”œâ”€â”€ âœ… Set up Jupyter notebooks
â””â”€â”€ âœ… Load Synthea dataset

Hour 3-4: Data Exploration
â”œâ”€â”€ âœ… Analyze each CSV file structure
â”œâ”€â”€ âœ… Identify COVID patients (conditions.csv)
â”œâ”€â”€ âœ… Map relationships between tables
â””â”€â”€ âœ… Generate data quality report

Hour 5-6: Core Data Pipeline
â”œâ”€â”€ âœ… Implement SyntheaLoader class
â”œâ”€â”€ âœ… Implement TimelineBuilder class
â”œâ”€â”€ âœ… Test on 10 sample patients
â””â”€â”€ âœ… Debug data linkage issues

Hour 7-8: Feature Engineering
â”œâ”€â”€ âœ… Implement FeatureEngineer class
â”œâ”€â”€ âœ… Create static features (demographics, comorbidities)
â”œâ”€â”€ âœ… Create temporal features (vitals, medications)
â””â”€â”€ âœ… Save processed data for Day 2
```

**Day 2: Model Development (8 hours)**
```
Hour 1-2: TFT Implementation
â”œâ”€â”€ âœ… Install pytorch-forecasting
â”œâ”€â”€ âœ… Implement COVID19TFT class
â”œâ”€â”€ âœ… Define multi-task heads
â””â”€â”€ âœ… Test forward pass

Hour 3-4: Loss Functions & Training
â”œâ”€â”€ âœ… Implement MultiTaskLoss class
â”œâ”€â”€ âœ… Implement COVID19TFTTrainer class
â”œâ”€â”€ âœ… Set up PyTorch Lightning callbacks
â””â”€â”€ âœ… Configure optimizers and schedulers

Hour 5-6: Data Pipeline Integration
â”œâ”€â”€ âœ… Create COVID19DataModule class
â”œâ”€â”€ âœ… Implement data loaders
â”œâ”€â”€ âœ… Handle variable sequence lengths
â””â”€â”€ âœ… Test training loop (1 epoch)

Hour 7-8: Baseline Models
â”œâ”€â”€ âœ… Implement LSTM baseline
â”œâ”€â”€ âœ… Implement simple logistic regression
â”œâ”€â”€ âœ… Ensure fair comparison setup
â””â”€â”€ âœ… Save model checkpoints
```

**Day 3: Training & Evaluation (8 hours)**
```
Hour 1-2: Hyperparameter Tuning
â”œâ”€â”€ âœ… Grid search key parameters
â”œâ”€â”€ âœ… Cross-validation setup
â”œâ”€â”€ âœ… Learning rate scheduling
â””â”€â”€ âœ… Early stopping configuration

Hour 3-4: Full Training Run
â”œâ”€â”€ âœ… Train TFT model (30-50 epochs)
â”œâ”€â”€ âœ… Train baseline models
â”œâ”€â”€ âœ… Monitor training metrics
â””â”€â”€ âœ… Save best model checkpoints

Hour 5-6: Model Evaluation
â”œâ”€â”€ âœ… Implement evaluation pipeline
â”œâ”€â”€ âœ… Generate performance metrics
â”œâ”€â”€ âœ… Statistical significance tests
â””â”€â”€ âœ… Create results tables

Hour 7-8: Interpretability Analysis
â”œâ”€â”€ âœ… Extract attention weights
â”œâ”€â”€ âœ… Analyze temporal patterns
â”œâ”€â”€ âœ… Feature importance analysis
â””â”€â”€ âœ… Generate visualizations
```

**Day 4: Results & Documentation (8 hours)**
```
Hour 1-2: Results Generation
â”œâ”€â”€ âœ… Generate all plots and tables
â”œâ”€â”€ âœ… Create attention heatmaps
â”œâ”€â”€ âœ… Clinical case studies
â””â”€â”€ âœ… Statistical analysis

Hour 3-4: Paper Writing
â”œâ”€â”€ âœ… Write abstract and introduction
â”œâ”€â”€ âœ… Complete methodology section
â”œâ”€â”€ âœ… Write results section
â””â”€â”€ âœ… Discussion and conclusion

Hour 5-6: Presentation Creation
â”œâ”€â”€ âœ… Create slide deck (10-12 slides)
â”œâ”€â”€ âœ… Practice presentation timing
â”œâ”€â”€ âœ… Record demonstration video
â””â”€â”€ âœ… Edit and finalize video

Hour 7-8: Final Deliverables
â”œâ”€â”€ âœ… Code documentation and cleanup
â”œâ”€â”€ âœ… Create GitHub repository
â”œâ”€â”€ âœ… Final paper formatting
â””â”€â”€ âœ… Submit all deliverables
```

#### 6.2 Critical Success Factors

**Technical Risk Mitigation:**
```
Risk 1: Data Processing Complexity
â”œâ”€â”€ Mitigation: Start with subset of patients (100-500)
â”œâ”€â”€ Fallback: Use simplified feature set
â””â”€â”€ Validation: Test pipeline on known data

Risk 2: Model Training Time
â”œâ”€â”€ Mitigation: Use smaller model initially
â”œâ”€â”€ Fallback: Pre-trained embeddings
â””â”€â”€ Optimization: Mixed precision training

Risk 3: Memory Requirements
â”œâ”€â”€ Mitigation: Batch size optimization
â”œâ”€â”€ Fallback: Gradient accumulation
â””â”€â”€ Hardware: Use GPU with sufficient VRAM

Risk 4: Convergence Issues
â”œâ”€â”€ Mitigation: Learning rate scheduling
â”œâ”€â”€ Fallback: Simpler architecture
â””â”€â”€ Debugging: Extensive logging
```

### 7. Code Quality and Standards

#### 7.1 Coding Standards
```python
# Type hints for all functions
def process_patient_timeline(
    patient_id: str, 
    tables: Dict[str, pd.DataFrame],
    config: Dict[str, Any]
) -> Tuple[pd.DataFrame, Dict[str, float]]:
    """
    Process patient timeline data.
    
    Args:
        patient_id: Unique patient identifier
        tables: Dictionary of loaded CSV tables
        config: Configuration parameters
        
    Returns:
        Tuple of processed timeline and metadata
    """
    pass

# Comprehensive error handling
try:
    timeline = builder.build_patient_timeline(patient_id)
except KeyError as e:
    logger.error(f"Missing data for patient {patient_id}: {e}")
    return None
except Exception as e:
    logger.error(f"Unexpected error processing {patient_id}: {e}")
    raise

# Logging throughout
import logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

logger.info(f"Processing {len(patients)} patients")
logger.debug(f"Feature dimensions: {features.shape}")
```

#### 7.2 Documentation Requirements
```python
# Class documentation
class COVID19TFT(nn.Module):
    """
    Temporal Fusion Transformer for COVID-19 severity prediction.
    
    This model combines static patient features (demographics, comorbidities)
    with time-varying clinical data (vitals, medications, procedures) to predict
    multiple severity outcomes including mortality, ICU admission, ventilator
    requirement, and length of stay.
    
    Architecture:
        - Static feature encoders for patient demographics
        - Temporal embeddings for time-varying features
        - Multi-head attention mechanism for temporal dependencies
        - Multi-task prediction heads for different outcomes
        
    Args:
        config (dict): Model configuration parameters
            - hidden_size: Hidden dimension size (default: 128)
            - num_attention_heads: Number of attention heads (default: 4)
            - dropout: Dropout probability (default: 0.1)
            
    Example:
        >>> config = {'hidden_size': 128, 'num_attention_heads': 4}
        >>> model = COVID19TFT(config)
        >>> predictions = model(batch_data)
    """
```

### 8. Performance Optimization

#### 8.1 Memory Optimization Strategies
```python
# Gradient checkpointing for memory efficiency
class COVID19TFT(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.use_checkpointing = config.get('gradient_checkpointing', True)
        
    def forward(self, x):
        if self.use_checkpointing and self.training:
            return checkpoint(self._forward_impl, x)
        else:
            return self._forward_impl(x)

# Dynamic batching based on sequence length
class COVID19DataLoader:
    def __init__(self, dataset, config):
        self.dataset = dataset
        self.batch_size = config['batch_size']
        
    def collate_fn(self, batch):
        # Sort by sequence length for efficient padding
        batch = sorted(batch, key=lambda x: x['sequence_length'], reverse=True)
        
        # Dynamic batch size based on max sequence length
        max_len = batch[0]['sequence_length']
        if max_len > 200:
            actual_batch_size = self.batch_size // 2
        else:
            actual_batch_size = self.batch_size
            
        return batch[:actual_batch_size]

# Mixed precision training
trainer = pl.Trainer(
    precision=16,  # Use FP16
    accumulate_grad_batches=2,  # Gradient accumulation
    gradient_clip_val=1.0
)
```

#### 8.2 Training Optimization
```python
# Learning rate finder
def find_optimal_lr(model, dataloader):
    """Find optimal learning rate using LR range test"""
    lr_finder = LRFinder(model, torch.optim.Adam(model.parameters()))
    lr_finder.range_test(dataloader, end_lr=1, num_iter=100)
    optimal_lr = lr_finder.history['lr'][np.argmin(lr_finder.history['loss'])]
    return optimal_lr

# Warmup and cosine annealing
def get_scheduler(optimizer, config):
    """Create learning rate scheduler with warmup"""
    warmup_steps = config.get('warmup_steps', 1000)
    total_steps = config.get('total_steps', 10000)
    
    def lr_lambda(step):
        if step < warmup_steps:
            return step / warmup_steps
        else:
            progress = (step - warmup_steps) / (total_steps - warmup_steps)
            return 0.5 * (1 + np.cos(np.pi * progress))
    
    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
```

### 9. Validation and Testing Framework

#### 9.1 Comprehensive Testing Suite
```python
# Unit tests for core components
class TestDataProcessing(unittest.TestCase):
    def setUp(self):
        self.mock_data = self.create_mock_synthea_data()
        
    def test_patient_timeline_creation(self):
        """Test timeline creation for various patient scenarios"""
        # Test normal patient
        timeline = create_patient_timeline('patient_001', self.mock_data)
        self.assertIsNotNone(timeline)
        self.assertGreater(len(timeline), 0)
        
        # Test patient with missing data
        timeline = create_patient_timeline('patient_missing', self.mock_data)
        self.assertIsNotNone(timeline)
        
        # Test patient with irregular timestamps
        timeline = create_patient_timeline('patient_irregular', self.mock_data)
        self.assertIsNotNone(timeline)

# Integration tests
class TestModelPipeline(unittest.TestCase):
    def test_end_to_end_pipeline(self):
        """Test complete pipeline from raw data to predictions"""
        # Load test data
        loader = SyntheaLoader('test_data/')
        tables = loader.load_all_tables()
        
        # Process single patient
        patient_id = 'test_patient_001'
        features = process_patient_data(patient_id, tables)
        
        # Model prediction
        model = COVID19TFT(test_config)
        predictions = model(features)
        
        # Validate outputs
        self.assertIn('mortality', predictions)
        self.assertTrue(0 <= predictions['mortality'] <= 1)

# Performance benchmarks
def benchmark_training_speed():
    """Benchmark training speed with different configurations"""
    configs = [
        {'batch_size': 16, 'hidden_size': 64},
        {'batch_size': 32, 'hidden_size': 128},
        {'batch_size': 64, 'hidden_size': 256}
    ]
    
    results = []
    for config in configs:
        start_time = time.time()
        train_one_epoch(config)
        elapsed = time.time() - start_time
        results.append({
            'config': config,
            'time_per_epoch': elapsed,
            'memory_usage': get_gpu_memory_usage()
        })
    
    return results
```

### 10. Deployment and Reproducibility

#### 10.1 Reproducibility Checklist
```python
# Set all random seeds
def set_random_seeds(seed: int = 42):
    """Set random seeds for reproducibility"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# Version tracking
def log_environment_info():
    """Log all relevant version information"""
    info = {
        'python_version': sys.version,
        'pytorch_version': torch.__version__,
        'transformers_version': transformers.__version__,
        'pandas_version': pd.__version__,
        'numpy_version': np.__version__,
        'cuda_version': torch.version.cuda,
        'gpu_name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'
    }
    
    with open('experiment_environment.json', 'w') as f:
        json.dump(info, f, indent=2)
    
    return info

# Experiment tracking
def track_experiment(config, results):
    """Track experiment configuration and results"""
    experiment = {
        'timestamp': datetime.now().isoformat(),
        'config': config,
        'results': results,
        'git_commit': get_git_commit_hash(),
        'environment': log_environment_info()
    }
    
    experiment_id = hashlib.md5(str(experiment).encode()).hexdigest()[:8]
    
    with open(f'experiments/{experiment_id}.json', 'w') as f:
        json.dump(experiment, f, indent=2)
    
    return experiment_id
```

#### 10.2 Final Deliverables Structure
```
Final Submission Package:
â”œâ”€â”€ README.md                          # Project overview and setup
â”œâ”€â”€ requirements.txt                   # Exact dependency versions
â”œâ”€â”€ environment.yml                    # Conda environment file
â”œâ”€â”€ src/                              # Source code
â”œâ”€â”€ notebooks/                        # Jupyter notebooks with results
â”œâ”€â”€ results/                          # Generated plots, tables, metrics
â”œâ”€â”€ models/                           # Trained model checkpoints
â”œâ”€â”€ paper/                            # ACM format paper (PDF + LaTeX)
â”œâ”€â”€ presentation/                     # Slides and recorded video
â”œâ”€â”€ data/                             # Processed data (not raw Synthea)
â””â”€â”€ experiments/                      # Experiment logs and configurations
```

This comprehensive design document provides everything needed to implement your TFT COVID-19 severity prediction project within the 4-day timeline while maintaining high code quality and research standards. 